{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains codes that can be used to reproduce the results reported in the paper:\n",
    "Reading differences in eye-tracking data as a marker of high functioning autism in adults: a machine-learning study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import linear_model\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.stats as st\n",
    "import sklearn\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def LoadTextDifficulties():\n",
    "    \"\"\"\n",
    "    Load difficulties for each text file into dictionaries\n",
    "    \n",
    "    difficulties1: the text difficulty measure according to how many users\n",
    "        answer questions about them correctly\n",
    "    \n",
    "    difficulties2: the text difficulty measure according to text complexity measures\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(\"./data/reading/t_difficulty_1.csv\",header=None)\n",
    "    \n",
    "    difficulties1 = {\n",
    "        x[0].replace(\"Text \",\"\").strip(): x[1] for x in df.values\n",
    "    }\n",
    "    \n",
    "    df = pd.read_csv(\"./data/reading/t_difficulty_2.csv\",header=None)\n",
    "    \n",
    "    difficulties2 = {\n",
    "        x[0].replace(\"T\",\"\").strip(): x[1] for x in df.values\n",
    "    }\n",
    "    \n",
    "    return difficulties1,difficulties2\n",
    "\n",
    "\n",
    "def GetAnswer(user_id, user_name, text_id, question_id):\n",
    "    \"\"\"\n",
    "    Get the answer of a particular question from an user\n",
    "    \n",
    "    Arguments:\n",
    "        user_id: the user id\n",
    "        user_name: the name of the user.\n",
    "        text_id: the text_id of the question\n",
    "        question_id: the question_id\n",
    "        \n",
    "        \n",
    "    Returns:\n",
    "        int: whether the question has been answered correctly (1) or incorrectly (0)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    q = question_id\n",
    "    \n",
    "    df = pd.read_csv(\"./data/reading/ParticipantAnswer_N.csv\")\n",
    "    \n",
    "    r = df[df.apply(lambda x: \n",
    "                    (x[\"User initials\"]+\"_\"+x[\"Group\"]==user_name.strip()) \n",
    "                    and (x[\" Text ID\"]==text_id) ,axis=1)][\"Score\"].values\n",
    "    \n",
    "    if((int(q)==0) or (int(q)==4)):\n",
    "        return sum([int(x) for x in r])/len(r)\n",
    "    \n",
    "    if (len(r)!=3):\n",
    "        return r[int(q)-1]\n",
    "    \n",
    "    return r[int(q)-1]\n",
    "   \n",
    "\n",
    "def GetData(file_name,text_set):\n",
    "    \"\"\"\n",
    "    Get the data given the file_name and text_set. \n",
    "    Only choose users that we have at least one data point for each of the text in the dataset \n",
    "    \n",
    "    \n",
    "    Returns: a tuple of \n",
    "        attentions_original: the dataframe containing all the data points of the chosen users\n",
    "        user_asd_original: list, the list of chosen asd users\n",
    "        user_control_original: list, the list of chosen control users\n",
    "    \"\"\"\n",
    "    \n",
    "    difficulties1,difficulties2 = LoadTextDifficulties()\n",
    "    \n",
    "    attentions = pd.read_csv(file_name)\n",
    "    attentions = attentions[attentions[\"Text ID\"].isin(text_set)]\n",
    "    attentions[\"Difficulty_1\"] = attentions[\"Text ID\"].apply(lambda x: difficulties1[str(int(x))])\n",
    "    attentions[\"Difficulty_2\"] = attentions[\"Text ID\"].apply(lambda x: difficulties2[str(int(x))])\n",
    "    \n",
    "    if(\"sentence\" not in file_name.lower()):\n",
    "    \n",
    "        attentions[\"Answer\"] = attentions.apply(\n",
    "            lambda x: GetAnswer(x[\" User ID\"], x[\" User Name\"], x[\"Text ID\"], x[\"Paragraph_Number\"]),axis=1)\n",
    "        \n",
    "    #attentions = attentions[~attentions[\" User Name\"].isin(ignored_users)]\n",
    "#attentions = attentions.sample(frac=1).reset_index(drop=True)\n",
    "    attentions_original=attentions[attentions[\" Time to 1st View (sec)\"]>0].reset_index(drop=True)\n",
    "    attentions_original = attentions_original[attentions_original[\" Time to 1st View (sec)\"]>0]\n",
    "    \n",
    "    if(\"Group\" not in attentions_original.columns):\n",
    "        attentions_original[\"Group\"] = attentions_original[\" User Name\"].apply(\n",
    "            lambda x: \"ASD\" if x.endswith(\"ASD\") else \"Control\")\n",
    "        \n",
    "    attentions_original[\"Group_bin\"] = attentions_original[\"Group\"].apply(\n",
    "        lambda x: 0 if x==\"ASD\" else 1)\n",
    "    attentions_original[\"Group_bin\"] = attentions_original[\"Group\"].apply(\n",
    "        lambda x: 0 if x==\"ASD\" else 1)\n",
    "    \n",
    "    #only choose users that we have at least one data point for each of the text in the dataset\n",
    "    text_id_count  = attentions_original[\n",
    "        [\"Text ID\", \" User ID\", \" User Name\"]].drop_duplicates().groupby(\n",
    "        \" User Name\", as_index=False\n",
    "    )[[\"Text ID\"]].count()\n",
    "    \n",
    "    good_text_id_count_users = text_id_count[text_id_count[\"Text ID\"]==len(text_set)][\" User Name\"]\n",
    "    \n",
    "    attentions_original = attentions_original[\n",
    "        attentions_original[\" User Name\"].isin(good_text_id_count_users)]\n",
    "    user_asd_original = attentions_original[\n",
    "        (attentions_original[\"Group\"]=='ASD')][\" User Name\"].unique()\n",
    "    user_control_original=attentions_original[\n",
    "        (attentions_original[\"Group\"]=='Control')][\" User Name\"].unique()\n",
    "    \n",
    "    if(len(set(user_asd_original).intersection(user_control_original))!=0):\n",
    "        attentions_original[\" User Name\"] = attentions_original.apply(\n",
    "            lambda x: x[\" User Name\"]+\"_\"+x[\"Group\"], axis=1)\n",
    "\n",
    "        user_asd_original = attentions_original[\n",
    "            attentions_original[\"Group\"]=='ASD'][\" User Name\"].unique()\n",
    "        user_control_original=attentions_original[\n",
    "            attentions_original[\"Group\"]=='Control'][\" User Name\"].unique()\n",
    "        \n",
    "    print(\"number of asd: {}; control: {}\".format(len(user_asd_original),len(user_control_original)))\n",
    "    \n",
    "    return attentions_original,user_asd_original,user_control_original\n",
    "\n",
    "\n",
    "def RunSingleExperiment(\n",
    "    attentions,\n",
    "    user_asd_original,\n",
    "    user_control_original,\n",
    "    n_user_test,\n",
    "    train_length,\n",
    "    n_folds,\n",
    "    one_hot_columns,eclf):\n",
    "    \"\"\"Run one experiment for a single set of feature.\n",
    "    \n",
    "      Arguments:\n",
    "        attentions: the dataframe of data points for the experiment.\n",
    "        user_asd_original: list, selected asd users to be included.\n",
    "        user_control_original: list, selected control users to be included.\n",
    "        n_user_test: int, the number of users in the test set for each group (half the total number of users used for test).\n",
    "        train_length: int, the number of users in the train set for each group (half the total number of users used for train).\n",
    "        n_folds: int, number of folds to run,\n",
    "        one_hot_columns: list, the list of additional features\n",
    "        eclf: a voting classifier ensemble\n",
    "        \n",
    "      Returns:\n",
    "        results_clf: results for each classifier in the ensemble, in the form of accuracy of each fold\n",
    "    \"\"\"\n",
    "    \n",
    "    features_one_hots = [\n",
    "        x for x in attentions.columns if any(x.startswith(y+\"_\") for y in one_hot_columns)]\n",
    "    \n",
    "    results_clf = {\n",
    "        x[0]:[] for x in eclf.estimators\n",
    "    }\n",
    "    \n",
    "    results_clf.update({\"ensemble\":[]})\n",
    "    \n",
    "    for i in range(0,n_folds):\n",
    "        #sample users for training and testing\n",
    "        user_asd_test = np.random.choice(user_asd_original,n_user_test)\n",
    "        user_asd_train = random.sample(\n",
    "            [x for x in user_asd_original if x not in user_asd_test],train_length)\n",
    "        \n",
    "        user_control_test = np.random.choice(user_control_original,n_user_test)\n",
    "        user_control_train = random.sample(\n",
    "            [x for x in user_control_original if x not in user_control_test],train_length)\n",
    "\n",
    "        #get the data for training and testing\n",
    "        attentions_train=attentions[\n",
    "            (((attentions[\" User Name\"].isin(user_asd_train)) & (attentions[\"Group\"]==\"ASD\"))|\n",
    "             ((attentions[\" User Name\"].isin(user_control_train)) & (attentions[\"Group\"]==\"Control\")))]\n",
    "\n",
    "        attentions_test=attentions[\n",
    "            (((attentions[\" User Name\"].isin(user_asd_test)) & (attentions[\"Group\"]==\"ASD\"))\n",
    "             |((attentions[\" User Name\"].isin(user_control_test)) & (attentions[\"Group\"]==\"Control\")))]\n",
    "        \n",
    "\n",
    "        attentions_train_X=attentions_train[features+features_one_hots]\n",
    "        attentions_train_Y=attentions_train[\"Group\"]\n",
    "        \n",
    "        attentions_test_X=attentions_test[features+features_one_hots]\n",
    "        attentions_test_Y=attentions_test[\"Group\"]\n",
    "        \n",
    "        eclf.fit(attentions_train_X,attentions_train_Y)\n",
    "        \n",
    "        attentions_test_Y_n = attentions_test_Y.apply(lambda x: list(eclf.classes_).index(x))\n",
    "        \n",
    "        for clf in eclf.named_estimators_:\n",
    "            #clfs[clf].fit(attentions_train_X,attentions_train_Y)\n",
    "            results=eclf.named_estimators_[clf].predict(attentions_test_X)==attentions_test_Y_n\n",
    "            train_outcome=[]\n",
    "        \n",
    "            for user in user_control_test:\n",
    "                user_asd_x=attentions[(attentions[\" User Name\"]==user) & (attentions[\"Group\"]==\"Control\")].index\n",
    "                r_x=results[user_asd_x].value_counts()\n",
    "                train_outcome.append(r_x.index[0])\n",
    "\n",
    "            for user in user_asd_test:\n",
    "                user_asd_x=attentions[(attentions[\" User Name\"]==user) & (attentions[\"Group\"]==\"ASD\")].index\n",
    "                r_x=results[user_asd_x].value_counts()\n",
    "                train_outcome.append(r_x.index[0])\n",
    "                \n",
    "            correct = sum(train_outcome)\n",
    "            results_clf[clf].append(correct)\n",
    "            \n",
    "        results=eclf.predict(attentions_test_X)==attentions_test_Y\n",
    "        \n",
    "        train_outcome=[]\n",
    "        \n",
    "        for user in user_control_test:\n",
    "            user_asd_x=attentions[(attentions[\" User Name\"]==user) & (attentions[\"Group\"]==\"Control\")].index\n",
    "            r_x=results[user_asd_x].value_counts()\n",
    "            train_outcome.append(r_x.index[0])\n",
    "\n",
    "        for user in user_asd_test:\n",
    "            user_asd_x=attentions[(attentions[\" User Name\"]==user) & (attentions[\"Group\"]==\"ASD\")].index\n",
    "            r_x=results[user_asd_x].value_counts()\n",
    "            train_outcome.append(r_x.index[0])\n",
    "        correct = sum(train_outcome)\n",
    "        results_clf[\"ensemble\"].append(correct)\n",
    "    #print(one_hot_columns,results_clf)\n",
    "\n",
    "    \n",
    "    for m in results_clf:\n",
    "        a = results_clf[m]\n",
    "        a = np.array(a)/n_user_test/2\n",
    "        results_clf[m] = a\n",
    "        \n",
    "        print(\n",
    "            one_hot_columns,\n",
    "            m, \n",
    "            np.mean(a), \n",
    "            st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a)),\n",
    "            sep=\"|\"\n",
    "        )\n",
    "        \n",
    "    return results_clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RunExperiments(file_name,text_set,n_user_test,n_folds,features,one_hot_columns_all,eclf):\n",
    "    \"\"\"\n",
    "    Run all permutations of feature sets for a particular AOI setting and text set\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "        file_name: string, input file name.\n",
    "        text_set: list, the list of text ID in the set.\n",
    "        n_user_test: int, number of test users for each group, -1: 33% test, 66% train\n",
    "        n_folds: int, number of folds to run,\n",
    "        one_hot_columns_all: list, the list of all possible additional features\n",
    "        eclf: a voting classifier ensemble\n",
    "    Returns:\n",
    "        grand_results: results for all feature sets\n",
    "    \"\"\"\n",
    "    \n",
    "    attentions_original,user_asd_original,user_control_original = GetData(file_name,text_set)\n",
    "    \n",
    "    grand_results = {}\n",
    "    \n",
    "    attentions = pd.get_dummies(attentions_original,columns=one_hot_columns_all)\n",
    "    \n",
    "    min_length = min(len(user_asd_original),len(user_control_original))\n",
    "    print(\"number of selected users: \", min_length*2)\n",
    "    n_user_test = round(min_length*0.33) if n_user_test == -1 else n_user_test\n",
    "    print(\"number of selected test users: \", n_user_test*2)\n",
    "    train_length = min_length - n_user_test\n",
    "    print(\"number of selected train users: \", train_length*2)\n",
    "    \n",
    "    for n_onehot in range(1,len(one_hot_columns_all)+1):\n",
    "        for one_hot_columns in list(itertools.combinations(one_hot_columns_all,n_onehot))+[()]:    \n",
    "            if(one_hot_columns in grand_results):\n",
    "                continue\n",
    "            grand_results[one_hot_columns] = RunSingleExperiment(attentions,\n",
    "                                              user_asd_original,\n",
    "                                              user_control_original,\n",
    "                                              n_user_test,\n",
    "                                              train_length,\n",
    "                                              n_folds,\n",
    "                                              one_hot_columns,\n",
    "                                              eclf)\n",
    "            \n",
    "    return grand_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run the experiments on all the files\n",
    "Take sometimes to run, so first test the notebook with n_folds=1, and then if it run fine, test it with n_folds=100\n",
    "\"\"\"\n",
    "one_hot_columns_by_AOI = {}\n",
    "one_hot_columns_by_AOI[\"general\"] = [\" AOI Name\", \"Text ID\"]#, \"Paragraph_Number\",\"Answer\",\"Difficulty_1\", \"Flag\"]\n",
    "one_hot_columns_by_AOI[\"Title_paragraphs\"] = one_hot_columns_by_AOI[\"general\"] +  [\n",
    "    \"Paragraph_Number\",\"Answer\",\"Difficulty_1\"]\n",
    "\n",
    "one_hot_columns_by_AOI[\"Questions\"] = one_hot_columns_by_AOI[\"general\"] +  [\"Paragraph_Number\",\"Answer\"]\n",
    "one_hot_columns_by_AOI[\"Sentences\"] = one_hot_columns_by_AOI[\"general\"] +  [\"Difficulty_1\"]\n",
    "\n",
    "one_hot_columns_by_AOI[\"Title_paragraphs_Questions_combined\"] = one_hot_columns_by_AOI[\"general\"] +  [\n",
    "    \"Paragraph_Number\",\"Answer\",\"Difficulty_1\",\"Flag\"]\n",
    "\n",
    "\n",
    "set_1 = [1,2,3,4,5,6,7,8,9]\n",
    "set_2 = [10,11,12,13,14,15,16,17]\n",
    "set_3 = [18,19,20]\n",
    "\n",
    "features = [' Time to 1st View (sec)', ' Time Viewed (sec)',\n",
    "        ' Fixations (#)', ' Revisits (#)']\n",
    "\n",
    "eclf = sklearn.ensemble.VotingClassifier(estimators=[\n",
    "    ('randomForest', RandomForestClassifier(n_estimators=100, n_jobs=-1)),\n",
    "    ('kNeigh', KNeighborsClassifier()), \n",
    "    ('svc', svm.SVC()), \n",
    "    ('logistic', linear_model.LogisticRegression(max_iter=10000)), \n",
    "    ('logisticCV', linear_model.LogisticRegressionCV(max_iter=10000)), \n",
    "    (\"XGBClassifier\",XGBClassifier(n_jobs=-1,nthreads=-1))],\n",
    "                                         voting='hard')\n",
    "import os\n",
    "\n",
    "input_files_by_AOI_setting = {}\n",
    "input_files_by_AOI_setting[\"Title_paragraphs\"] = \"./data/reading/Title_paragraph.csv\"\n",
    "input_files_by_AOI_setting[\"Questions\"] = \"./data/reading/Questions.csv\"\n",
    "input_files_by_AOI_setting[\"Sentences\"] = \"./data/reading/Sentence_aggregate.csv\"\n",
    "input_files_by_AOI_setting[\"Title_paragraphs_Questions_combined\"] = \"./data/reading/Text_question_combined.csv\"\n",
    "\n",
    "#Test the code with n_folds=1 first, and then set it to 100 for the final run. The final run could take up to 20 hours.\n",
    "n_folds = 1\n",
    "AOI_setting_results = {}\n",
    "\n",
    "for AOI_setting in input_files_by_AOI_setting:\n",
    "    \n",
    "    print(\"current AOI setting:\", AOI_setting)\n",
    "    file_name = input_files_by_AOI_setting[AOI_setting]\n",
    "    \n",
    "    text_set_results = {}\n",
    "    \n",
    "    for text_set in zip([\"set_1\",\"set_2\",\"set_3\"],[set_1,set_2,set_3]):\n",
    "        \n",
    "        (attentions_original,\n",
    "         user_asd_original,\n",
    "         user_control_original) = GetData(file_name,text_set[1])\n",
    "        \n",
    "        n_user_test = -1\n",
    "        attentions = pd.get_dummies(attentions_original,columns=one_hot_columns_by_AOI[AOI_setting])\n",
    "        \n",
    "        min_length = min(len(user_asd_original),len(user_control_original))\n",
    "        print(\"number of selected users: \", min_length*2)\n",
    "        n_user_test = round(min_length*0.33) if n_user_test == -1 else n_user_test\n",
    "        print(\"number of selected test users: \", n_user_test*2)\n",
    "        train_length = min_length - n_user_test\n",
    "        print(\"number of selected train users: \", train_length*2)\n",
    "        \n",
    "        one_hot_columns_set = [()]\n",
    "        for column in one_hot_columns_by_AOI[AOI_setting]:\n",
    "            one_hot_columns_set = one_hot_columns_set + [\n",
    "                x+(column,) for x in one_hot_columns_set]\n",
    "        \n",
    "        by_columns_results = {}\n",
    "        \n",
    "        for one_hot_columns in tqdm(one_hot_columns_set):\n",
    "            \n",
    "            by_columns_results[one_hot_columns] = RunSingleExperiment(\n",
    "                attentions,\n",
    "                user_asd_original,\n",
    "                user_control_original,\n",
    "                n_user_test,\n",
    "                train_length,\n",
    "                n_folds,\n",
    "                one_hot_columns,\n",
    "                eclf\n",
    "            )\n",
    "            \n",
    "        text_set_results[text_set[0]] = by_columns_results\n",
    "        \n",
    "    AOI_setting_results[AOI_setting] = text_set_results\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the results\n",
    "\"\"\"\n",
    "for_df = []\n",
    "for AOI_setting in input_files_by_AOI_setting:\n",
    "    for text_set in [\"set_1\",\"set_2\",\"set_3\"]:\n",
    "        \n",
    "        one_hot_columns_set = [()]\n",
    "        \n",
    "        for column in one_hot_columns_by_AOI[AOI_setting]:\n",
    "            one_hot_columns_set = one_hot_columns_set + [\n",
    "                x+ (column,) for x in one_hot_columns_set]\n",
    "        for one_hot_columns in one_hot_columns_set:\n",
    "            for ml in AOI_setting_results[AOI_setting][text_set][one_hot_columns]:\n",
    "                a = AOI_setting_results[AOI_setting][text_set][one_hot_columns][ml]\n",
    "                for_df.append ([\n",
    "                    text_set,\n",
    "                    AOI_setting,\n",
    "                    one_hot_columns,\n",
    "                    ml,\n",
    "                    np.mean(a), st.t.interval(0.95, len(a)-1, loc=np.mean(a), scale=st.sem(a))\n",
    "                ])\n",
    "                \n",
    "df_results = pd.DataFrame(for_df)\n",
    "df_results.columns = [\"text_set\",\"AOI_Setting\",\"Feature set\",\"ml\",\"mean_acc\",\"ci\"]\n",
    "results_filename = \"./results_test.csv\"\n",
    "df_results.to_csv(results_filename, index=None)\n",
    "            #break\n",
    "        #break\n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6\n",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
